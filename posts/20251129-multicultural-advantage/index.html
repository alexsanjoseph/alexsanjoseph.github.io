<!DOCTYPE html>
<html lang="en">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.127.0">

    
    
    

<title>Is there a Multicultural Advantage in AI? • Alex&#39;s Blog</title>



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Is there a Multicultural Advantage in AI?">
  <meta name="twitter:description" content="Pasting random things together">
      <meta name="twitter:site" content="@alexsanjoseph">

<meta property="og:url" content="/posts/20251129-multicultural-advantage/">
  <meta property="og:site_name" content="Alex&#39;s Blog">
  <meta property="og:title" content="Is there a Multicultural Advantage in AI?">
  <meta property="og:description" content="Pasting random things together">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-29T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-30T13:54:20+05:30">
    <meta property="article:tag" content="Ai">
    <meta property="article:tag" content="India">
    <meta property="article:tag" content="Languages">
    <meta property="article:tag" content="Data">


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/monokai.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.18569620416fd38f5ffded3a82ed40831401fa9567742b967cb8a5b974799132.css" integrity="sha256-GFaWIEFv049f/e06gu1AgxQB&#43;pVndCuWfLiluXR5kTI=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <link rel="stylesheet" href="/css/sidebar-toggle.css">
    <link rel="stylesheet" href="/css/site-home-link.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="icon" href="/icon.png">
    <link rel="shortcut icon" href="/icon.png">
    
    

    <script src="/js/sidebar-toggle.js"></script>
</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="/">Alex&#39;s Blog</a>
      </span>
      
        
        
        
        <div class="author-image">
          <img src="/images/alex.jpeg" alt="Author Image" class="img--circle img--headshot element--center">
        </div>
        
      
      
      <p class="site__description">
         Pasting random things together 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Alex&#39;s Blog</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/portfolio/">
						<span>Portfolio</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	<a href="https://twitter.com/alexsanjoseph" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://github.com/alexsanjoseph" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/alexsanjoseph" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="https://stackoverflow.com/users/1653808/alex-joseph" rel="me"><i class="fab fa-stack-overflow fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
	<a href="mailto:alexsanjoseph%20at%20gmail.com" rel="me"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

      </div>
    </div>
    
<div class="copyright">
  &copy; 2016 - 2025 alexsanjoseph
  
</div>


<div class="builtwith">
Built with <a href="https://gohugo.io">Hugo</a> ❤️ <a href="https://github.com/htr3n/hyde-hyde">hyde-hyde</a>.
</div>


  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <div class="home-button">
      <a href="/" class="home-link">← Home</a>
    </div>
    
    <div class="post-header-image">
      <img src="/images/multicultural-advantage/thinking_in_hindi.jpg" alt="Is there a Multicultural Advantage in AI?" class="header-image" />
    </div>
    
    <h1>Is there a Multicultural Advantage in AI?</h1>
    
    
<div class="post__meta">
    </script>
    
    
    <i class="fas fa-calendar-alt"></i> Created: 2025 Nov 29
    
    
    
    , Updated:
    2025 Nov 30
    
    
    
    
    
    
    
    <br />
     <i class="fas fa-tags"></i>
    
    <a class="badge badge-tag" href="/tags/ai">ai</a>
     
    
    <a class="badge badge-tag" href="/tags/india">india</a>
     
    
    <a class="badge badge-tag" href="/tags/languages">languages</a>
     
    
    <a class="badge badge-tag" href="/tags/data">data</a>
    
    
    
    
    <br />
    <i class="fas fa-clock"></i> 7 min read
</div>

  </header>
  
  
  <div class="post">
    <blockquote class="info">
    <strong>This post ventures into the land of pure hypothesis. Caveat Emptor</strong><br>
    
</blockquote>
<p>When GPT-5 launched, <a href="https://www.androidauthority.com/gpt-5-vs-4o-usage-impressions-3586126/">everyone</a> <a href="https://www.zdnet.com/article/i-tested-gpt-5s-coding-skills-and-it-was-so-bad-that-im-sticking-with-gpt-4o-for-now/">on</a> <a href="https://futurism.com/theory-why-gpt-5-sucks">the</a> <a href="https://garymarcus.substack.com/p/gpt-5-overdue-overhyped-and-underwhelming">internet</a> (with the notable exception of one Mr. Altman) was disappointed by how uninspiring it was. The general consensus is that AI progress with the current structure of LLMs is plateauing. GPT 5.1/Opus 4.5/Gemini Pro might be beating benchmarks, but true novel problems don&rsquo;t seem to be getting solved. This is apparently happening because we&rsquo;ve supposedly scraped all the quality text on the internet. Every book ever written, ever Wikipedia article, every Stack Overflow answer, and every Reddit thread (which caused all the big content aggregators to lock down further content without paying the piper) have already been digested by AI.</p>
<p>According to Richard Sutton&rsquo;s <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">Bitter Lesson</a>, computation and data beat clever algorithms over the long run. However, this also means that if we are trying to use just data to make AI better, once you are out of new data you&rsquo;ve painted yourself into a corner.</p>
<h2 id="is-there-an-anglo-centric-bottleneck">Is there an Anglo-Centric Bottleneck?</h2>
<p>Nobody really understands how LLMs became such amazing worldbuilders, just by reading more and more text. However, it is clear that the world that was built was primarily Anglo-centric, since that&rsquo;s where most of the data comes from. This means that the concepts and the connections that LLMs might have are also heavily skewed to the English language world.</p>
<p>However, the &ldquo;exhausted internet&rdquo; only applies to English. Maybe throw in some European languages if we&rsquo;re being generous.</p>
<p>But there&rsquo;s a ridiculous amount of untapped data across languages spoken in India, China, Southeast Asia, Africa, and Latin America. Conversations happening across WhatsApp groups in Hindi, technical discussions in Mandarin forums, legal reasoning in Bengali, agricultural knowledge in Tamil - none of this has been properly captured.</p>
<p>Where this is different is that this isn&rsquo;t just &ldquo;more of the same data.&rdquo; Different languages encode fundamentally different ways of thinking about the world.</p>
<h2 id="hello-sapir-whorf-hypothesis-meet-transformers">Hello Sapir-Whorf Hypothesis, meet Transformers</h2>
<p>It might be easy to think of languages as just different vocabularies mapped onto the same concepts. However, it might not be this simple, since there are <a href="https://en.wikipedia.org/wiki/Linguistic_relativity">good arguments</a> that different languages are fundamentally different ways of thinking about the world.</p>
<p>Take Turkish. Every time you make a statement, you have to specify whether you saw it directly, heard it from someone, or inferred it. There&rsquo;s no way around it - the grammar forces you to mark how you got this information. Compare that to English where &ldquo;it rained yesterday&rdquo; could mean anything from &ldquo;I was soaked&rdquo; to &ldquo;someone told me&rdquo; to &ldquo;the ground is wet.&rdquo;</p>
<p>Or spatial navigation: some languages don&rsquo;t have &ldquo;left&rdquo; and &ldquo;right&rdquo; at all. Instead, speakers of languages like Guugu Yimithirr use absolute directions (north/south/east/west) for everything. &ldquo;Move the cup a bit north&rdquo; instead of &ldquo;move the cup to the right.&rdquo; This fundamentally changes how speakers think about and navigate space.</p>
<p>Why does this matters for AI improvements? Training on diverse languages doesn&rsquo;t just add more facts to the pile. It creates a higher-dimensional latent space that allows for better reasoning. An AI that understands both &ldquo;linear time&rdquo; (English: &ldquo;looking forward to the future&rdquo;) and &ldquo;vertical time&rdquo; (Mandarin: &ldquo;shàng ge yuè&rdquo; = &ldquo;up month&rdquo; = last month) isn&rsquo;t just bilingual. Or Hindi, where &ldquo;kal&rdquo; means both yesterday and tomorrow - the same word for opposite temporal directions, disambiguated only by context.</p>
<blockquote class="info">
    <strong>NOTE: the above examples are from Wikipedia/ChatGPT, I DO NOT speak these languages myself!</strong><br>
    
</blockquote>
<p>Each language gives the model a different geometric representation of the same abstract concept, which means it can reason about &ldquo;time&rdquo; from multiple angles simultaneously. If you only train a model on English descriptions of &ldquo;time,&rdquo; you get some of the axes in the latent space. Add Mandarin and you get many more axes that are genuinely orthogonal. The model can now navigate a richer conceptual space.</p>
<h2 id="different-cultures-are-different-worlds">Different Cultures are Different Worlds</h2>
<p>This goes deeper than linguistic structure. Different cultures have different ways of thinking about the universe. I will give a few examples from my part of the world.</p>
<p>The undisputed GOAT of mathematical intuition, <a href="https://en.wikipedia.org/wiki/Srinivasa_Ramanujan">Srinivasa Ramanujan</a>, famously arrived at theorems through pure thought - he&rsquo;d see the answer first, then work backwards to find the proof (and many a time don&rsquo;t even bother to). Western mathematics goes the opposite direction: axioms → theorems → proofs. He had very little formal training, but obviously had a fundamentally different way of navigating mathematical manifold. Could an AI trained on both Indian and Western traditions take advantage of this and find mathematical proofs out of thin air a la Ramanujan?</p>
<p>There are WhatsApp university posts that claim Sanskrit is the best language to program computers because of Panini&rsquo;s Ashtadhyayi - a 2,500-year-old Sanskrit grammar textbook that&rsquo;s essentially a manual to a formal programming language. It uses recursion, metarules, and a sophisticated notation system to describe linguistic rules. The claim is obvious hyperbole - Python is hard enough already, forget about memorizing the twelve inflections of &ldquo;Rama&rdquo; just to write a for-loop. But an AI trained on Sanskrit could connect underlying grammatical structure to code in genuinely different computational paradigms.</p>
<p>Legal reasoning is another example. In Western legal traditions, &ldquo;law&rdquo; is a set of codified rules. In Indian legal philosophy, &ldquo;Dharma&rdquo; is context-dependent - the same action can be righteous or unrighteous depending on the situation, your role, and the consequences. An AI that understands both approaches has a richer model of how humans think about right and wrong.</p>
<p>Right now, are AI systems overfitting to a particularly Anglo-centric view of the world?</p>
<h2 id="there-are-good-reasons-why-nobody-has-done-this-yet">There are good reasons why nobody has done this yet</h2>
<p>Of course, training LLMs natively like this is not easy. <strong>The first problem is just getting the data.</strong> The valuable stuff isn&rsquo;t sitting in public GitHub repos - it&rsquo;s in WhatsApp groups, family forwards, regional forums, PDF scans of old legal documents, and oral histories that aren&rsquo;t even digitized.</p>
<p><strong>But even if you could scrape every WhatsApp forward, you&rsquo;d hit a second problem:</strong> current LLM tokenizers are optimized for English and Latin scripts. A single Hindi word might get split into multiple tokens, making the model far less efficient. Languages with complex scripts like Tamil or Bengali need fundamentally different tokenization strategies - different attention mechanisms, language-specific adapter layers, the whole works. <a href="https://www.sarvam.ai/">Companies</a> are still figuring out how to do this.</p>
<p><strong>The third problem is that nobody actually speaks in neat, separated languages.</strong> Multilingual speakers code-switch constantly - &ldquo;<em>Yaar</em>, I was at the market today and the <em>bhai</em> was like&hellip;&rdquo; That fluidity is messy and hard to label, and it&rsquo;s exactly the kind of data that would be most valuable.</p>
<p><strong>And even if you solve all of that,</strong> how do you prove your model is better? Current benchmarks are Anglo-centric as well! A model that&rsquo;s genuinely better at reasoning about Dharma or understanding Hindi humor won&rsquo;t show up on MMLU scores. You&rsquo;d need entirely new evaluation frameworks, and good luck getting those accepted by the global research community.</p>
<h2 id="an-opportunity-for-the-global-south">An opportunity for the &ldquo;Global South&rdquo;?</h2>
<p>However, if you can actually show real value this way, does this mean that countries with more heterogeneous languages (and cultures) have an inherent advantage?</p>
<p>I honestly don&rsquo;t know, but it is undeniable that the current LLMs have captured the internet of the last 30 years. Across the world, there have been civilizations and worldviews two orders of magnitude older and more diverse than just the English-speaking world, but they are trapped in PDFs, oral histories, and dialects.</p>
<p>Maybe China already understands this. They&rsquo;re building models trained heavily on Chinese data, for Chinese use cases, with Chinese cultural context. The US had a head start because the internet was English-first. But that advantage could be temporary.</p>
<p>India could be next. The diversity within the country is greater than almost any other nation or area (bar African nations, who face even more structural constraints). We&rsquo;re talking about 22 official languages, hundreds of dialects, and multiple writing systems - all encoding different cultural knowledge.</p>
<p>So can India or other diverse nations actually capitalize on this? I have no idea. However, it is clear that the current approach of training bigger models on the same English-heavy internet has diminishing returns. If the next breakthrough in AI comes from genuinely different ways of thinking about the world, then the advantage goes to whoever can unlock that diversity first.</p>
<p>The West has the infrastructural advantage but that is already a commodity. What isn&rsquo;t commoditizing (because it is hard!) is 1000s of languages, 10000 years of philosophical traditions, and fundamentally different ways of encoding knowledge. That might matter more than anyone realizes.</p>
  </div>
  


  

  
    
        <div id="graphcomment"></div>
<script type="text/javascript">
  window.graphcomment_id = 'blog-alexsanjoseph-com';
   
  (function() {
    var gc = document.createElement('script'); gc.type = 'text/javascript'; gc.async = true;
    gc.src = 'https://graphcomment.com/js/integration.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(gc);
  })();
</script>
    


</article>

        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.11.2/js/all.js"
    integrity="sha384-b3ua1l97aVGAPEIe48b4TC60WUQbQaGi2jqAWM90y0OZXZeyaTCWtBTKtjW2GXG1"
    crossorigin="anonymous"></script>
<script data-goatcounter="https://alexsanjoseph.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
     inlineMath: [['$','$'], ['\\(','\\)']],
     displayMath: [['$$','$$']],
     processEscapes: true,
     processEnvironments: true,
     skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
     TeX: { equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>
<script>
    
    
    
    window.goatcounter.visit_count({ append: 'body' })
</script>

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    



    



    </body>
</html>
